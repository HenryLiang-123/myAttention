# myAttention
Repo for my implementation of multiheaded attention from the original paper Attention is All You Need.

## Note

I did not invent this. This is me trying to understand the encoder-decoder architecture.

## References

[Code](https://www.youtube.com/watch?v=U0s0f995w14)

[Original Paper](https://arxiv.org/abs/1706.03762)
